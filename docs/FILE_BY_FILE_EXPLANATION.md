# ğŸ“ íŒŒì¼ë³„ ìƒì„¸ ì½”ë“œ ì„¤ëª… - ì½”í‹€ë¦° ì´ˆë³´ììš©

## ğŸ¯ ì´ ë¬¸ì„œì˜ ëª©ì 
ì½”í‹€ë¦°ì„ ì˜ ëª¨ë¥´ëŠ” ë¶„ë„ **ê° íŒŒì¼ì´ ë¬´ì—‡ì„ í•˜ëŠ”ì§€** ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## 1ï¸âƒ£ **MainActivity.kt** - ì•±ì˜ ì‹œì‘ì  ğŸš€

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- ì•±ì´ ì¼œì§€ë©´ **ê°€ì¥ ë¨¼ì € ì‹¤í–‰**ë˜ëŠ” íŒŒì¼
- í™”ë©´ì„ ê·¸ë¦¬ê³ , ê¶Œí•œì„ ìš”ì²­í•˜ê³ , ë‹¤ë¥¸ ëª¨ë“  ì‹œìŠ¤í…œì„ ì‹œì‘ì‹œí‚´

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        // ğŸ“± ì•±ì´ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜
        super.onCreate(savedInstanceState)
        setContent {
            // ğŸ¨ í™”ë©´ UIë¥¼ ê·¸ë¦¬ê¸° ì‹œì‘
            XRTESTTheme {
                if (LocalSpatialCapabilities.current.isSpatialUiEnabled) {
                    // âœ¨ VR/AR ëª¨ë“œ (3D í™”ë©´)
                    MySpatialContent()
                } else {
                    // ğŸ“± ì¼ë°˜ ëª¨ë“œ (í‰ë©´ í™”ë©´)  
                    My2DContent()
                }
            }
        }
    }
}
```

### **ì£¼ìš” í•¨ìˆ˜ë“¤**:
- `onCreate()`: ì•± ì‹œì‘í•  ë•Œ í•œ ë²ˆ ì‹¤í–‰
- `My2DContent()`: ì¼ë°˜ ìŠ¤ë§ˆíŠ¸í° í™”ë©´ UI ê·¸ë¦¬ê¸°
- `MySpatialContent()`: VR/AR 3D ê³µê°„ UI ê·¸ë¦¬ê¸°

### **ì—¬ê¸°ì„œ ì‹œì‘ë˜ëŠ” ê²ƒë“¤**:
1. ì¹´ë©”ë¼ ë§¤ë‹ˆì € ì´ˆê¸°í™”
2. ìŒì„± ë§¤ë‹ˆì € ì´ˆê¸°í™”  
3. OpenAI ë¹„ì „ í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
4. ê¶Œí•œ ìš”ì²­ (ì¹´ë©”ë¼, ë§ˆì´í¬)

---

## 2ï¸âƒ£ **VisionIntegration.kt** - ì „ì²´ ì‹œìŠ¤í…œ ì§€íœ˜ê´€ ğŸ¯

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **ëª¨ë“  ì‹œìŠ¤í…œì„ í†µí•© ê´€ë¦¬**í•˜ëŠ” í•µì‹¬ íŒŒì¼
- ì¹´ë©”ë¼ + ìŒì„± + OpenAIë¥¼ í•˜ë‚˜ë¡œ ì—°ê²°

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class VisionIntegration(
    private val context: Context,
    private val apiKey: String,           // OpenAI API í‚¤
    private val camera2Manager: Camera2Manager,  // ì¹´ë©”ë¼ ì‹œìŠ¤í…œ
    private val voiceManager: VoiceManager       // ìŒì„± ì‹œìŠ¤í…œ
) {
    
    fun startSession() {
        // ğŸ”— OpenAIì— ì—°ê²° ì‹œì‘
        realtimeClient.connect()
        _state.value = IntegrationState.CONNECTING
    }
    
    fun sendQuery(question: String) {
        // ğŸ“¸ ì¹´ë©”ë¼ì—ì„œ í˜„ì¬ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        val currentFrame = camera2Manager.getCurrentFrame()
        
        // ğŸ¤– OpenAIì—ê²Œ ì´ë¯¸ì§€ + ì§ˆë¬¸ ì „ì†¡
        realtimeClient.sendImageWithPrompt(currentFrame, question)
        
        _state.value = IntegrationState.PROCESSING
    }
}
```

### **ìƒíƒœ ê´€ë¦¬**:
```kotlin
enum class IntegrationState {
    IDLE,           // ğŸ˜´ ëŒ€ê¸° ì¤‘
    CONNECTING,     // ğŸ”Œ OpenAI ì—°ê²° ì¤‘
    READY,          // âœ… ì¤€ë¹„ ì™„ë£Œ
    LISTENING,      // ğŸ‘‚ ìŒì„± ì…ë ¥ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘
    PROCESSING,     // ğŸ¤– AIê°€ ìƒê°í•˜ëŠ” ì¤‘
    RESPONDING,     // ğŸ’¬ AIê°€ ë‹µë³€í•˜ëŠ” ì¤‘
    ERROR           // âŒ ì˜¤ë¥˜ ë°œìƒ
}
```

### **ì´ íŒŒì¼ì˜ ì—­í• **:
1. **í†µí•© ê´€ë¦¬**: ëª¨ë“  ì‹œìŠ¤í…œì„ í•˜ë‚˜ë¡œ ì—°ê²°
2. **ìƒíƒœ ì¶”ì **: í˜„ì¬ ì‹œìŠ¤í…œì´ ë­˜ í•˜ê³  ìˆëŠ”ì§€ ê´€ë¦¬
3. **ë°ì´í„° ì „ë‹¬**: ì¹´ë©”ë¼ â†’ OpenAI â†’ ìŒì„± ì¶œë ¥ ì—°ê²°

---

## 3ï¸âƒ£ **RealtimeVisionClient.kt** - OpenAIì™€ ëŒ€í™”í•˜ëŠ” ë‹´ë‹¹ì ğŸ¤–

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **OpenAI GPT-4V APIì™€ ì‹¤ì‹œê°„ WebSocket í†µì‹ **
- ì´ë¯¸ì§€ + ìŒì„±ì„ ë³´ë‚´ê³ , AI ë‹µë³€ì„ ë°›ì•„ì˜´

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class RealtimeVisionClient(
    private val apiKey: String,
    private val onAudioResponse: (ByteArray) -> Unit,  // ìŒì„± ì‘ë‹µ ì½œë°±
    private val onTextResponse: (String) -> Unit,      // í…ìŠ¤íŠ¸ ì‘ë‹µ ì½œë°±
    private val onError: (String) -> Unit              // ì—ëŸ¬ ì½œë°±
) {
    
    fun connect() {
        // ğŸŒ OpenAI ì„œë²„ì— WebSocket ì—°ê²°
        val request = Request.Builder()
            .url("wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17")
            .addHeader("Authorization", "Bearer $apiKey")
            .addHeader("OpenAI-Beta", "realtime=v1")
            .build()
        
        webSocket = client.newWebSocket(request, createWebSocketListener())
    }
    
    fun sendImageWithPrompt(imageData: ByteArray, prompt: String?) {
        // ğŸ“¸ ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ë³€í™˜
        val base64Image = Base64.encodeToString(imageData, Base64.NO_WRAP)
        
        // ğŸ“¤ OpenAIì—ê²Œ ì „ì†¡í•  JSON ë©”ì‹œì§€ ë§Œë“¤ê¸°
        val event = JSONObject().apply {
            put("type", "conversation.item.create")
            put("item", JSONObject().apply {
                put("type", "message")
                put("role", "user")
                put("content", JSONArray().apply {
                    // ì´ë¯¸ì§€ ì¶”ê°€
                    put(JSONObject().apply {
                        put("type", "input_image")
                        put("image", base64Image)
                    })
                    // í…ìŠ¤íŠ¸ ì§ˆë¬¸ ì¶”ê°€
                    if (!prompt.isNullOrEmpty()) {
                        put(JSONObject().apply {
                            put("type", "input_text")
                            put("text", prompt)
                        })
                    }
                })
            })
        }
        
        // ğŸš€ ì „ì†¡!
        sendEvent(event)
        requestResponse() // AIì—ê²Œ ë‹µë³€ ìš”ì²­
    }
}
```

### **ì´ë²¤íŠ¸ ì²˜ë¦¬**:
```kotlin
private fun handleRealtimeEvent(event: JSONObject) {
    val type = event.getString("type")
    
    when (type) {
        "response.audio.delta" -> {
            // ğŸ”Š ìŒì„± ì‘ë‹µ ì¡°ê°ì´ ë„ì°©í•¨
            val delta = event.getString("delta")
            val audioData = Base64.decode(delta, Base64.DEFAULT)
            // ìŒì„± ì¬ìƒ ëŒ€ê¸°ì—´ì— ì¶”ê°€
        }
        
        "response.text.delta" -> {
            // ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ì¡°ê°ì´ ë„ì°©í•¨
            val delta = event.getString("delta")
            onTextResponse(delta) // UIì— í…ìŠ¤íŠ¸ í‘œì‹œ
        }
        
        "error" -> {
            // âŒ ì—ëŸ¬ ë°œìƒ
            val error = event.getJSONObject("error")
            val message = error.getString("message")
            onError(message)
        }
    }
}
```

### **ì´ íŒŒì¼ì˜ í•µì‹¬**:
1. **WebSocket ì—°ê²°**: OpenAIì™€ ì‹¤ì‹œê°„ ì†Œí†µ
2. **ì´ë¯¸ì§€ ì „ì†¡**: ì¹´ë©”ë¼ ì‚¬ì§„ì„ AIì—ê²Œ ë³´ëƒ„
3. **ì‘ë‹µ ìˆ˜ì‹ **: AIì˜ ìŒì„±+í…ìŠ¤íŠ¸ ë‹µë³€ ë°›ê¸°
4. **ì—ëŸ¬ ì²˜ë¦¬**: ì—°ê²° ëŠê¹€, API ì˜¤ë¥˜ ë“± ì²˜ë¦¬

---

## 4ï¸âƒ£ **AudioStreamManager.kt** - ê³ í’ˆì§ˆ ìŒì„± ì²˜ë¦¬ ğŸ”Š

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **24kHz ê³ í’ˆì§ˆ ìŒì„±** ë…¹ìŒ ë° ì¬ìƒ
- OpenAI APIì— ë§ëŠ” ì •í™•í•œ ìŒì„± í¬ë§· ì²˜ë¦¬

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class AudioStreamManager(
    private val onAudioCaptured: (ByteArray) -> Unit
) {
    companion object {
        private const val SAMPLE_RATE = 24000      // 24kHz (OpenAI ìš”êµ¬ì‚¬í•­)
        private const val CHANNEL_CONFIG_RECORD = AudioFormat.CHANNEL_IN_MONO
        private const val CHANNEL_CONFIG_PLAY = AudioFormat.CHANNEL_OUT_MONO
        private const val AUDIO_FORMAT = AudioFormat.ENCODING_PCM_16BIT  // 16ë¹„íŠ¸
    }
    
    fun startRecording() {
        // ğŸ¤ ë§ˆì´í¬ì—ì„œ 24kHzë¡œ ë…¹ìŒ ì‹œì‘
        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            SAMPLE_RATE,
            CHANNEL_CONFIG_RECORD,
            AUDIO_FORMAT,
            bufferSize
        )
        
        audioRecord?.startRecording()
        
        // ğŸ”„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ê³„ì† ë…¹ìŒ
        recordingJob = coroutineScope.launch {
            while (isRecording) {
                val buffer = ByteArray(bufferSize)
                val bytesRead = audioRecord?.read(buffer, 0, buffer.size) ?: 0
                
                if (bytesRead > 0) {
                    // ğŸ“Š ë…¸ì´ì¦ˆ ì œê±° ë° ìŒëŸ‰ ì •ê·œí™”
                    val processedAudio = processAudioInput(buffer)
                    onAudioCaptured(processedAudio)
                }
            }
        }
    }
    
    fun playAudio(audioData: ByteArray) {
        // ğŸ”Š 24kHz ìŒì„±ì„ ìŠ¤í”¼ì»¤ë¡œ ì¬ìƒ
        audioTrack = AudioTrack(
            AudioManager.STREAM_MUSIC,
            SAMPLE_RATE,
            CHANNEL_CONFIG_PLAY,
            AUDIO_FORMAT,
            audioData.size,
            AudioTrack.MODE_STREAM
        )
        
        audioTrack?.play()
        audioTrack?.write(audioData, 0, audioData.size)
    }
}
```

### **ìŒì„± ì²˜ë¦¬ ê³¼ì •**:
1. **ë…¹ìŒ**: ë§ˆì´í¬ â†’ 24kHz PCM16 í˜•ì‹
2. **ë…¸ì´ì¦ˆ ì œê±°**: ë°°ê²½ ì†ŒìŒ í•„í„°ë§
3. **ì •ê·œí™”**: ìŒëŸ‰ ì ì • ìˆ˜ì¤€ìœ¼ë¡œ ì¡°ì •
4. **ì „ì†¡**: OpenAIì—ê²Œ Base64ë¡œ ì¸ì½”ë”©í•´ì„œ ì „ì†¡
5. **ìˆ˜ì‹ **: OpenAIì—ì„œ 24kHz ìŒì„± ì‘ë‹µ ë°›ê¸°
6. **ì¬ìƒ**: ìŠ¤í”¼ì»¤ë¡œ ê³ í’ˆì§ˆ ìŒì„± ì¶œë ¥

---

## 5ï¸âƒ£ **Camera2Manager.kt** - ì¹´ë©”ë¼ ì „ë¬¸ê°€ ğŸ“·

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **Android Camera2 APIë¡œ ì¹´ë©”ë¼ ì œì–´**
- ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ì§„ì„ ì°ì–´ì„œ AI ë¶„ì„ìš©ìœ¼ë¡œ ì œê³µ

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class Camera2Manager(private val context: Context) {
    
    fun startCamera() {
        // ğŸ“· ì¹´ë©”ë¼ ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°
        val cameraManager = context.getSystemService(Context.CAMERA_SERVICE) as CameraManager
        
        try {
            // ğŸ“‹ í›„ë©´ ì¹´ë©”ë¼ ì°¾ê¸°
            val cameraId = cameraManager.cameraIdList.first { id ->
                val characteristics = cameraManager.getCameraCharacteristics(id)
                characteristics.get(CameraCharacteristics.LENS_FACING) == 
                    CameraCharacteristics.LENS_FACING_BACK
            }
            
            // ğŸ”§ ì¹´ë©”ë¼ ì„¤ì •
            cameraManager.openCamera(cameraId, cameraDeviceCallback, backgroundHandler)
            
        } catch (e: Exception) {
            Log.e(TAG, "Camera initialization failed: ${e.message}")
        }
    }
    
    private fun createCameraPreviewSession() {
        // ğŸ“¸ ì—°ì† ì‚¬ì§„ ì´¬ì˜ì„ ìœ„í•œ ì„¸ì…˜ ë§Œë“¤ê¸°
        val reader = ImageReader.newInstance(
            PREVIEW_WIDTH,    // 1920í”½ì…€
            PREVIEW_HEIGHT,   // 1080í”½ì…€  
            ImageFormat.JPEG, // JPEG í¬ë§·
            2                 // ìµœëŒ€ 2ì¥ ë²„í¼
        )
        
        reader.setOnImageAvailableListener({ reader ->
            // ğŸ“¸ ìƒˆ ì‚¬ì§„ì´ ì°í ë•Œë§ˆë‹¤ ì‹¤í–‰
            val image = reader.acquireLatestImage()
            processFrame(image) // ì´ë¯¸ì§€ ì²˜ë¦¬
            image.close()
        }, backgroundHandler)
    }
    
    private fun processFrame(image: Image) {
        // ğŸ–¼ï¸ ì´ë¯¸ì§€ë¥¼ JPEG ë°”ì´íŠ¸ ë°°ì—´ë¡œ ë³€í™˜
        val buffer = image.planes[0].buffer
        val bytes = ByteArray(buffer.remaining())
        buffer.get(bytes)
        
        // ğŸ“ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (OpenAI ìš”êµ¬ì‚¬í•­: ìµœëŒ€ 1024x1024)
        val resizedBytes = resizeImageForAPI(bytes)
        
        // ğŸ“¤ VisionIntegrationì—ê²Œ ì „ë‹¬
        onFrameProcessed(resizedBytes)
    }
}
```

### **ì¹´ë©”ë¼ ì‘ì—… ìˆœì„œ**:
1. **ê¶Œí•œ í™•ì¸**: ì¹´ë©”ë¼ ì‚¬ìš© ê¶Œí•œ ì²´í¬
2. **ì¹´ë©”ë¼ ì—´ê¸°**: í›„ë©´ ì¹´ë©”ë¼ í™œì„±í™”
3. **í”„ë¦¬ë·° ì‹œì‘**: ì‹¤ì‹œê°„ ì˜ìƒ í‘œì‹œ
4. **ìë™ ì´¬ì˜**: 1ì´ˆë§ˆë‹¤ ì‚¬ì§„ ì´¬ì˜
5. **ì´ë¯¸ì§€ ì²˜ë¦¬**: í¬ê¸° ì¡°ì •, í¬ë§· ë³€í™˜
6. **ì „ë‹¬**: VisionIntegrationìœ¼ë¡œ ì´ë¯¸ì§€ ì „ì†¡

---

## 6ï¸âƒ£ **VoiceManager.kt** - ìŒì„± ì¸ì‹ ì „ë¬¸ê°€ ğŸ¤

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜** (Speech-to-Text)
- **í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜** (Text-to-Speech) - ë°±ì—…ìš©

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
class VoiceManager(private val context: Context) {
    
    private var speechRecognizer: SpeechRecognizer? = null
    private var textToSpeech: TextToSpeech? = null
    
    fun startListening() {
        // ğŸ§ ìŒì„± ì¸ì‹ ì‹œì‘
        speechRecognizer = SpeechRecognizer.createSpeechRecognizer(context)
        
        val intent = Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH).apply {
            putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, 
                    RecognizerIntent.LANGUAGE_MODEL_FREE_FORM)
            putExtra(RecognizerIntent.EXTRA_LANGUAGE, Locale.getDefault())
            putExtra(RecognizerIntent.EXTRA_PARTIAL_RESULTS, true) // ì‹¤ì‹œê°„ ê²°ê³¼
        }
        
        speechRecognizer?.setRecognitionListener(object : RecognitionListener {
            override fun onResults(results: Bundle?) {
                // âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ!
                val matches = results?.getStringArrayList(SpeechRecognizer.RESULTS_RECOGNITION)
                if (!matches.isNullOrEmpty()) {
                    val recognizedText = matches[0]
                    _recognizedText.value = recognizedText
                    Log.d(TAG, "Recognized: $recognizedText")
                }
            }
            
            override fun onError(error: Int) {
                // âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨
                Log.e(TAG, "Speech recognition error: $error")
                _recognizedText.value = null
            }
        })
        
        speechRecognizer?.startListening(intent)
    }
    
    fun speak(text: String) {
        // ğŸ—£ï¸ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜ (ë°±ì—…ìš©)
        textToSpeech?.speak(
            text,
            TextToSpeech.QUEUE_FLUSH, // ì´ì „ ìŒì„± ì¤‘ë‹¨í•˜ê³  ìƒˆë¡œ ì¬ìƒ
            null,
            "utteranceId"
        )
    }
}
```

### **ìŒì„± ì²˜ë¦¬ íë¦„**:
1. **ë“£ê¸° ì‹œì‘**: `startListening()` í˜¸ì¶œ
2. **ìŒì„± ìˆ˜ì‹ **: ë§ˆì´í¬ë¡œ ì‚¬ìš©ì ìŒì„± ì…ë ¥
3. **í…ìŠ¤íŠ¸ ë³€í™˜**: "ì´ê²Œ ë­ì•¼?" â†’ í…ìŠ¤íŠ¸
4. **ê²°ê³¼ ì „ë‹¬**: MainActivityì—ì„œ ì¸ì‹ëœ í…ìŠ¤íŠ¸ ê°ì§€
5. **ì§ˆë¬¸ ì „ì†¡**: VisionIntegration.sendQuery() í˜¸ì¶œ

---

## 7ï¸âƒ£ **CrosshairOverlay.kt** - ì‹­ìê°€ UI ğŸ¯

### **ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼**:
- **í™”ë©´ ì¤‘ì•™ì— ì‹­ìê°€ í‘œì‹œ**
- ì‹œìŠ¤í…œ ìƒíƒœì— ë”°ë¼ ìƒ‰ìƒê³¼ ì• ë‹ˆë©”ì´ì…˜ ë³€ê²½

### **í•µì‹¬ ì½”ë“œ ë¶€ë¶„**:
```kotlin
@Composable
fun CrosshairOverlay(
    isActive: Boolean = true,
    isTargeting: Boolean = false,
    modifier: Modifier = Modifier
) {
    Box(modifier = modifier) {
        if (isActive) {
            // âŠ• ì‹­ìê°€ ê·¸ë¦¬ê¸°
            Canvas(
                modifier = Modifier
                    .size(60.dp)
                    .align(Alignment.Center)
            ) {
                val crosshairColor = when {
                    isTargeting -> Color(0xFFFFA500)  // ğŸŸ  ì£¼í™©ìƒ‰ (ì²˜ë¦¬ ì¤‘)
                    else -> Color.White               // âšª í°ìƒ‰ (ëŒ€ê¸°)
                }
                
                val strokeWidth = 3.dp.toPx()
                val crossSize = 30.dp.toPx()
                
                // ì„¸ë¡œì„  ê·¸ë¦¬ê¸°
                drawLine(
                    color = crosshairColor,
                    start = Offset(center.x, center.y - crossSize/2),
                    end = Offset(center.x, center.y + crossSize/2),
                    strokeWidth = strokeWidth
                )
                
                // ê°€ë¡œì„  ê·¸ë¦¬ê¸°  
                drawLine(
                    color = crosshairColor,
                    start = Offset(center.x - crossSize/2, center.y),
                    end = Offset(center.x + crossSize/2, center.y),
                    strokeWidth = strokeWidth
                )
                
                // ê°€ìš´ë° ì 
                drawCircle(
                    color = crosshairColor,
                    radius = 4.dp.toPx(),
                    center = center
                )
            }
        }
    }
}
```

### **ì‹­ìê°€ ìƒíƒœ**:
- **âšª í°ìƒ‰**: ëŒ€ê¸° ìƒíƒœ - "ì¡°ì¤€í•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”"
- **ğŸŸ  ì£¼í™©ìƒ‰**: ì²˜ë¦¬ ì¤‘ - "AIê°€ ë¶„ì„í•˜ê³  ìˆì–´ìš”"
- **ğŸ”´ ë¹¨ê°„ìƒ‰**: ì—ëŸ¬ ìƒíƒœ - "ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”"

---

## ğŸŒ **ë„¤íŠ¸ì›Œí¬ íŒŒì¼ë“¤**

### **A2AClient.kt** - Agent í†µì‹  ë‹´ë‹¹ì
```kotlin
// ë‹¤ë¥¸ Agentë“¤(Perception, UX/TTS, Logger)ê³¼ HTTPë¡œ í†µì‹ 
class A2AClient {
    suspend fun sendMessage(agentPort: Int, message: String): String {
        // HTTP POSTë¡œ JSON-RPC 2.0 ë©”ì‹œì§€ ì „ì†¡
    }
}
```

### **A2AModels.kt** - ë°ì´í„° êµ¬ì¡° ì •ì˜
```kotlin
// Agent ê°„ í†µì‹ ì— ì‚¬ìš©í•  ë°ì´í„° í˜•ì‹ë“¤
data class A2AMessage(
    val messageId: String,
    val taskId: String,
    val contextId: String,
    val parts: List<A2APart>
)
```

---

## âš™ï¸ **ì„¤ì • íŒŒì¼ë“¤**

### **build.gradle.kts** - ë¹Œë“œ ì„¤ì •
```kotlin
dependencies {
    // ğŸ“· ì¹´ë©”ë¼ ë¼ì´ë¸ŒëŸ¬ë¦¬
    implementation("androidx.camera:camera-camera2:1.4.0")
    
    // ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¼ì´ë¸ŒëŸ¬ë¦¬  
    implementation("com.squareup.okhttp3:okhttp:4.12.0")
    
    // ğŸ” JSON ì²˜ë¦¬
    implementation("org.json:json:20230618")
    
    // OpenAI API í‚¤ ì„¤ì •
    buildConfigField("String", "OPENAI_API_KEY", "\"${project.findProperty("OPENAI_API_KEY")}\"")
}
```

### **AndroidManifest.xml** - ê¶Œí•œ ì„¤ì •
```xml
<!-- ğŸ“· ì¹´ë©”ë¼ ê¶Œí•œ -->
<uses-permission android:name="android.permission.CAMERA" />

<!-- ğŸ¤ ë§ˆì´í¬ ê¶Œí•œ -->
<uses-permission android:name="android.permission.RECORD_AUDIO" />

<!-- ğŸŒ ì¸í„°ë„· ê¶Œí•œ -->
<uses-permission android:name="android.permission.INTERNET" />
```

---

## ğŸš€ **ì „ì²´ ì‹¤í–‰ íë¦„ ìš”ì•½**

1. **MainActivity** â†’ ì•± ì‹œì‘, UI ê·¸ë¦¬ê¸°
2. **ê¶Œí•œ ìš”ì²­** â†’ ì¹´ë©”ë¼, ë§ˆì´í¬ ê¶Œí•œ
3. **VisionIntegration** â†’ ì „ì²´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
4. **RealtimeVisionClient** â†’ OpenAI ì—°ê²°
5. **ëŒ€ê¸° ìƒíƒœ** â†’ LISTENING, ì‹­ìê°€ í°ìƒ‰
6. **ì‚¬ìš©ì ì§ˆë¬¸** â†’ VoiceManager ìŒì„± ì¸ì‹
7. **ì´ë¯¸ì§€ ìº¡ì²˜** â†’ Camera2Manager ì‚¬ì§„ ì´¬ì˜  
8. **AI ìš”ì²­** â†’ ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ë¥¼ OpenAIì— ì „ì†¡
9. **AI ì‘ë‹µ** â†’ 24kHz ìŒì„±ìœ¼ë¡œ ë‹µë³€ ìˆ˜ì‹ 
10. **ìŒì„± ì¬ìƒ** â†’ AudioStreamManagerë¡œ ê³ í’ˆì§ˆ ì¬ìƒ
11. **ì™„ë£Œ** â†’ ë‹¤ì‹œ LISTENING ìƒíƒœë¡œ

**ì´ì œ ê° íŒŒì¼ì´ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì™„ì „íˆ ì´í•´í•˜ì…¨ë‚˜ìš”? ğŸ¯**